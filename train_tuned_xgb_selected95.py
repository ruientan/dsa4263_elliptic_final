"""
Train tuned XGBoost (engineered_weight) using the SHAP-selected features
that together explain 95% of total |SHAP| importance.
"""

import os
import json
import pickle
import numpy as np
import pandas as pd
import xgboost as xgb
import random

from sklearn.metrics import (
    precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, confusion_matrix
)

from data_loader import EllipticDataLoader
from feature_eng import FeatureEngineer

SEED = 42
np.random.seed(SEED)
random.seed(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)

# -------------------------------------------------------------------
# 1. Load 95%-cumulative SHAP feature list (generated by SHAP script)
# -------------------------------------------------------------------
SELECTED_PATH = "results/shap_selected95_xgb_tuned.csv"
if not os.path.exists(SELECTED_PATH):
    raise FileNotFoundError(
        "Run the SHAP script with 95% cumulative selection first to "
        "generate shap_selected95_xgb_tuned.csv"
    )

selected_df = pd.read_csv(SELECTED_PATH)
selected_features = selected_df["feature"].tolist()

print(f"Loaded {len(selected_features)} SHAP-selected features (95% cumulative):")
for f in selected_features:
    print("  -", f)

# -------------------------------------------------------------------
# 2. Load engineered dataset (full 194 features, same as main pipeline)
# -------------------------------------------------------------------
print("\nLoading engineered feature matrices...")
loader = EllipticDataLoader(data_dir="data")
xgb_data = loader.get_feature_matrix_for_xgboost(use_engineered=True)

X_train_full = pd.DataFrame(xgb_data["X_train"])
X_test_full  = pd.DataFrame(xgb_data["X_test"])
y_train = xgb_data["y_train"]
y_test  = xgb_data["y_test"]

# -------------------------------------------------------------------
# 3. Recover feature names (must match how SHAP was computed)
# -------------------------------------------------------------------
tx_features, tx_classes, _, addr_tx_in, tx_addr_out = loader.load_raw_data()
engineer = FeatureEngineer("data")
df_eng, eng_features = engineer.engineer_features(
    tx_features, tx_classes, addr_tx_in, tx_addr_out
)

exclude_cols = ['txId', 'Time step', 'time_step', 'class', 'label', 'out_range_ratio']
all_features = df_eng.drop(columns=exclude_cols, errors="ignore")
numeric_features = all_features.select_dtypes(include=[np.number]).columns.tolist()

# Make sure DataFrame columns match feature names used in SHAP
X_train_full.columns = numeric_features
X_test_full.columns = numeric_features

# -------------------------------------------------------------------
# 4. Reduce to 95%-SHAP-selected feature subset
# -------------------------------------------------------------------
print("\nSelecting SHAP 95%-cumulative feature subset...")
missing = [f for f in selected_features if f not in X_train_full.columns]
if missing:
    raise ValueError(f"The following selected features are missing in X matrices: {missing}")

X_train = X_train_full[selected_features].values
X_test  = X_test_full[selected_features].values

print("Reduced X_train shape:", X_train.shape)
print("Reduced X_test shape :", X_test.shape)

# -------------------------------------------------------------------
# 5. Load tuned hyperparameters (same as best tuned XGBoost)
# -------------------------------------------------------------------
PARAM_PATH = "results/xgb_tuning_results.json"
if not os.path.exists(PARAM_PATH):
    raise FileNotFoundError(
        "Run XGBoost tuning first to generate xgb_tuning_results.json"
    )

with open(PARAM_PATH, "r") as f:
    tuned_json = json.load(f)

best_params = tuned_json["best_params"]

print("\nLoaded tuned hyperparameters:")
for k, v in best_params.items():
    print(f"  {k}: {v}")

# -------------------------------------------------------------------
# 6. Initialize tuned XGBoost model (same params, fewer features)
# -------------------------------------------------------------------
model = xgb.XGBClassifier(
    objective="binary:logistic",
    eval_metric="logloss",
    random_state=42,
    tree_method="hist",
    **best_params
)

print("\nTraining XGBoost (SHAP-selected 95% features)...")
model.fit(X_train, y_train)

# -------------------------------------------------------------------
# 7. Evaluate performance on test set
# -------------------------------------------------------------------
print("\nEvaluating on test set...")

y_prob = model.predict_proba(X_test)[:, 1]
y_pred = (y_prob >= 0.5).astype(int)

precision = precision_score(y_test, y_pred, zero_division=0)
recall = recall_score(y_test, y_pred, zero_division=0)
f1 = f1_score(y_test, y_pred, zero_division=0)
auc_pr = average_precision_score(y_test, y_prob)
auc_roc = roc_auc_score(y_test, y_prob)
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()

print("\n=== XGBOOST (TUNED, SHAP 95%-FEATURE SUBSET) PERFORMANCE ===")
print(f"Precision : {precision:.4f}")
print(f"Recall    : {recall:.4f}")
print(f"F1        : {f1:.4f}")
print(f"AUC-PR    : {auc_pr:.4f}")
print(f"AUC-ROC   : {auc_roc:.4f}")
print(f"TP={tp}, FP={fp}, FN={fn}, TN={tn}")

# -------------------------------------------------------------------
# 8. Save trained model
# -------------------------------------------------------------------
os.makedirs("models", exist_ok=True)
OUT_MODEL = "models/xgboost_engineered_weight_tuned_shap95.pkl"

with open(OUT_MODEL, "wb") as f:
    pickle.dump(model, f)

print(f"\nSaved SHAP-95% tuned model → {OUT_MODEL}")

# -------------------------------------------------------------------
# 9. Save result row (for comparison tables)
# -------------------------------------------------------------------
os.makedirs("results", exist_ok=True)
results_df = pd.DataFrame([{
    "model": "xgboost_tuned_shap95",
    "n_features": len(selected_features),
    "precision": precision,
    "recall": recall,
    "f1": f1,
    "auc_pr": auc_pr,
    "auc_roc": auc_roc,
    "tp": tp,
    "fp": fp,
    "fn": fn,
    "tn": tn
}])

results_df.to_csv("results/xgb_shap95_results.csv", index=False)
print("Saved results → results/xgb_shap95_results.csv")

print("\nDone.")
